{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules for cleansing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Packages.DataPreperation import CleanData\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob as txt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datetime import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn import svm\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0            1         2  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                                   3  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentData = pd.read_csv(r\"Data\\twitter_training.csv\", header=None)\n",
    "sentimentData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for the distribution of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Negative    22542\n",
       "Positive    20832\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentiment2 = sentimentData\n",
    "Sentiment2[2].loc[(Sentiment2[2] == \"positive\")] = 1\n",
    "Sentiment2[2].loc[(Sentiment2[2] == \"negative\")] = 0\n",
    "Sentiment2 = Sentiment2[~Sentiment2[2].str.contains(\"Neutral\")]\n",
    "Sentiment2 = Sentiment2[~Sentiment2[2].str.contains(\"Irrelevant\")]\n",
    "\n",
    "Sentiment2[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sentiment2[2].value_counts().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleansing initialised.\n"
     ]
    }
   ],
   "source": [
    "perform = CleanData()\n",
    "\n",
    "removeEmojis = True\n",
    "CleanText = True\n",
    "removeStopWords = True\n",
    "lemmatizeText = True\n",
    "POSLemmatizeTextFor = False\n",
    "Stemtextfor = False\n",
    "textcol = 3\n",
    "\n",
    "if CleanText == True:\n",
    "    Sentiment2 = perform.GeneralCleansingFor(Sentiment2, textcol)\n",
    "if removeEmojis == True:\n",
    "    Sentiment2 = perform.RemoveEmoticonsFor(Sentiment2)\n",
    "if removeStopWords == True:\n",
    "    Sentiment2 = perform.RemoveStopWordsFor(Sentiment2)\n",
    "if lemmatizeText == True:\n",
    "    Sentiment2 = perform.LemmatizeTextFor(Sentiment2)\n",
    "if POSLemmatizeTextFor == True:\n",
    "    Sentiment2 = perform.POSLemmatizeTextFor(Sentiment2)\n",
    "if Stemtextfor == True:\n",
    "    Sentiment2 = perform.StemTextFor(Sentiment2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform tokenisation and create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r\"[a-zA-Z0-9]+\")\n",
    "cv = CountVectorizer(stop_words=\"english\",ngram_range=(1,1),tokenizer=token.tokenize)\n",
    "text_counts = cv.fit_transform(Sentiment2[\"newReview\"])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, Sentiment2[2], test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 87.54%\n",
      "precision: 88.0\n",
      "recall: 87.0\n",
      "F1: 88.0\n",
      "confusion matrix: \n",
      "[[5027  596]\n",
      " [ 755 4466]]\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train,Y_train)\n",
    "predicted = MNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print(\"accuracy: \" + str('{:4.2f}'.format(accuracy_score*100))+'%')\n",
    "\n",
    "macro_precision = (metrics.precision_score(Y_test, predicted, average='macro'))\n",
    "macro_precision = (round(macro_precision,2))*100\n",
    "print(\"precision: \" + str(macro_precision))\n",
    "\n",
    "macro_recall = (metrics.recall_score(Y_test, predicted, average='macro'))\n",
    "macro_recall = (round(macro_recall,2))*100\n",
    "print(\"recall: \" + str(macro_recall))\n",
    "\n",
    "macro_f1 = (metrics.f1_score(Y_test, predicted, average='macro'))\n",
    "macro_f1 = (round(macro_f1,2))*100\n",
    "print(\"F1: \" + str(macro_f1))\n",
    "\n",
    "conf_mat = confusion_matrix(Y_test, predicted)\n",
    "print(\"confusion matrix: \\n\" + str(conf_mat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  88.952 %\n",
      "89.0\n",
      "89.0\n",
      "89.0\n",
      "[[4959  664]\n",
      " [ 534 4687]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver = 'liblinear', random_state = 42, max_iter=1000)\n",
    "lr.fit(X_train,Y_train)\n",
    "lg_pred = lr.predict(X_test)\n",
    "print(\"Accuracy: \",round((metrics.accuracy_score(Y_test,lg_pred))*100,3),\"%\")\n",
    "\n",
    "macro_precision = (metrics.precision_score(Y_test, lg_pred, average='macro'))\n",
    "macro_precision = (round(macro_precision,2))*100\n",
    "print(macro_precision)\n",
    "\n",
    "macro_recall = (metrics.recall_score(Y_test, lg_pred, average='macro'))\n",
    "macro_recall = (round(macro_recall,2))*100\n",
    "print(macro_recall)\n",
    "\n",
    "macro_f1 = (metrics.f1_score(Y_test, lg_pred, average='macro'))\n",
    "macro_f1 = (round(macro_f1,2))*100\n",
    "print(macro_f1)\n",
    "\n",
    "conf_mat = confusion_matrix(Y_test, lg_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "try wordvec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix for k = 1 is:\n",
      "\n",
      "[[5198  425]\n",
      " [ 240 4981]]\n",
      "\n",
      "Classification Report for k = 1 is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.96      0.92      0.94      5623\n",
      "    Positive       0.92      0.95      0.94      5221\n",
      "\n",
      "    accuracy                           0.94     10844\n",
      "   macro avg       0.94      0.94      0.94     10844\n",
      "weighted avg       0.94      0.94      0.94     10844\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix for k = 2 is:\n",
      "\n",
      "[[5300  323]\n",
      " [ 481 4740]]\n",
      "\n",
      "Classification Report for k = 2 is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.94      0.93      5623\n",
      "    Positive       0.94      0.91      0.92      5221\n",
      "\n",
      "    accuracy                           0.93     10844\n",
      "   macro avg       0.93      0.93      0.93     10844\n",
      "weighted avg       0.93      0.93      0.93     10844\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix for k = 3 is:\n",
      "\n",
      "[[5029  594]\n",
      " [ 272 4949]]\n",
      "\n",
      "Classification Report for k = 3 is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.95      0.89      0.92      5623\n",
      "    Positive       0.89      0.95      0.92      5221\n",
      "\n",
      "    accuracy                           0.92     10844\n",
      "   macro avg       0.92      0.92      0.92     10844\n",
      "weighted avg       0.92      0.92      0.92     10844\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix for k = 4 is:\n",
      "\n",
      "[[5251  372]\n",
      " [ 512 4709]]\n",
      "\n",
      "Classification Report for k = 4 is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.93      0.92      5623\n",
      "    Positive       0.93      0.90      0.91      5221\n",
      "\n",
      "    accuracy                           0.92     10844\n",
      "   macro avg       0.92      0.92      0.92     10844\n",
      "weighted avg       0.92      0.92      0.92     10844\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix for k = 5 is:\n",
      "\n",
      "[[4997  626]\n",
      " [ 385 4836]]\n",
      "\n",
      "Classification Report for k = 5 is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.89      0.91      5623\n",
      "    Positive       0.89      0.93      0.91      5221\n",
      "\n",
      "    accuracy                           0.91     10844\n",
      "   macro avg       0.91      0.91      0.91     10844\n",
      "weighted avg       0.91      0.91      0.91     10844\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix for k = 6 is:\n",
      "\n",
      "[[5125  498]\n",
      " [ 640 4581]]\n",
      "\n",
      "Classification Report for k = 6 is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.91      0.90      5623\n",
      "    Positive       0.90      0.88      0.89      5221\n",
      "\n",
      "    accuracy                           0.90     10844\n",
      "   macro avg       0.90      0.89      0.89     10844\n",
      "weighted avg       0.90      0.90      0.89     10844\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix for k = 7 is:\n",
      "\n",
      "[[4739  884]\n",
      " [ 412 4809]]\n",
      "\n",
      "Classification Report for k = 7 is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.84      0.88      5623\n",
      "    Positive       0.84      0.92      0.88      5221\n",
      "\n",
      "    accuracy                           0.88     10844\n",
      "   macro avg       0.88      0.88      0.88     10844\n",
      "weighted avg       0.88      0.88      0.88     10844\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix for k = 8 is:\n",
      "\n",
      "[[4912  711]\n",
      " [ 679 4542]]\n",
      "\n",
      "Classification Report for k = 8 is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.87      0.88      5623\n",
      "    Positive       0.86      0.87      0.87      5221\n",
      "\n",
      "    accuracy                           0.87     10844\n",
      "   macro avg       0.87      0.87      0.87     10844\n",
      "weighted avg       0.87      0.87      0.87     10844\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix for k = 9 is:\n",
      "\n",
      "[[4533 1090]\n",
      " [ 519 4702]]\n",
      "\n",
      "Classification Report for k = 9 is:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.81      0.85      5623\n",
      "    Positive       0.81      0.90      0.85      5221\n",
      "\n",
      "    accuracy                           0.85     10844\n",
      "   macro avg       0.85      0.85      0.85     10844\n",
      "weighted avg       0.86      0.85      0.85     10844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "prec_list = []\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "for k in range(1,10):\n",
    "\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k,algorithm='brute') #Using brute-force algorithm for quicker computation.\n",
    "    classifier.fit(X_train, Y_train) #Fitting the built-in sklearn classifier on our training data\n",
    "    predicted_label = classifier.predict(X_test) #Making the classifier to predict on the previously unseen test data.\n",
    "\n",
    "    accuracy_score = (metrics.accuracy_score(Y_test,predicted_label))\n",
    "    accuracy_score = (round(accuracy_score,2))*100\n",
    "    acc_list.append(accuracy_score)\n",
    "\n",
    "    confusion_mat = confusion_matrix(Y_test, predicted_label)\n",
    "    class_report = classification_report(Y_test, predicted_label)\n",
    "\n",
    "    macro_precision = (metrics.precision_score(Y_test, predicted_label, average='macro'))\n",
    "    macro_precision = (round(macro_precision,2))*100\n",
    "    prec_list.append(macro_precision)\n",
    "\n",
    "    macro_recall = (metrics.recall_score(Y_test, predicted_label, average='macro'))\n",
    "    macro_recall = (round(macro_recall,2))*100\n",
    "    rec_list.append(macro_recall)\n",
    "    \n",
    "    macro_f1 = (metrics.f1_score(Y_test, predicted_label, average='macro'))\n",
    "    macro_f1 = (round(macro_f1,2))*100\n",
    "    f1_list.append(macro_f1)\n",
    "\n",
    "    print(\"\\n\\nConfusion Matrix for k = {} is:\\n\".format(k))\n",
    "    print(confusion_mat)\n",
    "    print(\"\\nClassification Report for k = {} is:\\n\".format(k))\n",
    "    print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9014201401696791\n"
     ]
    }
   ],
   "source": [
    "svmModel = svm.SVC(kernel='linear')\n",
    "\n",
    "svmModel.fit(X_train, Y_train)\n",
    "\n",
    "svmPred = svmModel.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, svmPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_57116/3146715331.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmaxWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    154\u001b[0m           \u001b[1;32mor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m   \"\"\"\n\u001b[1;32m--> 156\u001b[1;33m   return sequence.pad_sequences(\n\u001b[0m\u001b[0;32m    157\u001b[0m       \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m       padding=padding, truncating=truncating, value=value)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`sequences` must be iterable.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;31m# non-zeros is more important.  For now, raise an exception!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\u001b[0m\u001b[0;32m    292\u001b[0m                         \" or shape[0]\")\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "maxWords = 500\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxWords)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxWords)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
